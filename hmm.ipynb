{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPhw3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNPnXo6Vrw-z"
      },
      "source": [
        "# NLP HW 3: Hidden Markov Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSJPMjXWDVeU"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaO6nRdwZ5qW"
      },
      "source": [
        "Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj46rvwvoci3"
      },
      "source": [
        "# from https://medium.com/@Keshav31/colab-features-download-and-upload-e1ec537a83df\n",
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "url = 'https://ttic.uchicago.edu/~kgimpel/teaching/31210-s19/data/31210-s19-hw3.zip'\n",
        "file = '31210-s19-hw3.zip'\n",
        "\n",
        "if not os.path.isfile(file):\n",
        "    urlretrieve(url,file)\n",
        "\n",
        "with ZipFile(file) as zipf:\n",
        "    zipf.extractall()\n",
        "\n",
        "!rm -rf data/\n",
        "!mkdir data/\n",
        "!mv 31210-s19-hw3/* data/\n",
        "!rm -rf 31210-s19-hw3.zip 31210-s19-hw3/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6SG96QHZ07o"
      },
      "source": [
        "Loading training data and doing bigram counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwButzrDrhp3",
        "outputId": "d9fae278-66b4-4ef5-bcb4-fc6fde7e0d00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from collections import Counter\n",
        "    \n",
        "word_tag_counts = Counter()\n",
        "tag_bigram_counts = Counter()\n",
        "vocab = set()\n",
        "pos_tags = set()\n",
        "    \n",
        "start_token = ('<s>', '<s>')\n",
        "end_token = ('</s>', '</s>')\n",
        "    \n",
        "with open('data/en_ewt.train', 'r') as trainfile:\n",
        "    prev = start_token\n",
        "    for row in trainfile.readlines():\n",
        "        row = row.split()\n",
        "        if len(row) == 0:\n",
        "            row = end_token\n",
        "        elif prev[0] == '</s>': \n",
        "            prev = start_token\n",
        "        if row[0] not in ['<s>', '</s>']:\n",
        "            vocab.add(row[0])\n",
        "            pos_tags.add(row[1])\n",
        "        word_tag_counts[(row[0], row[1])] += 1\n",
        "        tag_bigram_counts[(prev[1], row[1])] += 1\n",
        "        prev = row\n",
        "\n",
        "print('data loaded')\n",
        "print('vocab size:', str(len(vocab)))\n",
        "print('num pos tags:', str(len(pos_tags)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data loaded\n",
            "vocab size: 19380\n",
            "num pos tags: 50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5m5-uARC0J-"
      },
      "source": [
        "## 1. Supervised Parameter Estimation for HMMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2g_Z04wfgIc"
      },
      "source": [
        "### a) Compute probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pshhjt7tffOX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "log_p_tau = {}\n",
        "lambda_tau = 0.1\n",
        "\n",
        "for y_p in list(pos_tags):\n",
        "    denom = sum([tag_bigram_counts[(y_p, y_pp)] + lambda_tau\n",
        "                       for y_pp in list(pos_tags) + ['</s>']])\n",
        "    \n",
        "    for y in list(pos_tags) + ['</s>']:\n",
        "        numer = tag_bigram_counts[(y_p, y)]\n",
        "        log_p_tau[(y, y_p)] = np.log((numer + lambda_tau)/denom)\n",
        "\n",
        "for y in list(pos_tags):\n",
        "    denom = sum([tag_bigram_counts[('<s>', y_pp)] + lambda_tau\n",
        "                        for y_pp in list(pos_tags)])\n",
        "    numer = tag_bigram_counts[('<s>', y)]\n",
        "    log_p_tau[(y, '<s>')] = np.log((numer + lambda_tau)/denom)\n",
        "\n",
        "log_p_eta = {}\n",
        "lambda_eta = 0.001\n",
        "for y in list(pos_tags):\n",
        "    denom = sum([word_tag_counts[(x_p, y)] + lambda_eta\n",
        "                       for x_p in list(vocab)])\n",
        "    for x in list(vocab):\n",
        "        numer = word_tag_counts[(x, y)]\n",
        "        log_p_eta[(x, y)] = np.log((numer + lambda_eta)/denom)\n",
        "\n",
        "# make sure that all the zero probs are right\n",
        "log_p_tau[('</s>', '<s>')] = -float('inf')\n",
        "for y in list(pos_tags):\n",
        "    log_p_tau[('<s>', y)] = -float('inf')\n",
        "    log_p_tau[(y, '</s>')] = -float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcst_uQAAyT_"
      },
      "source": [
        "### b) Check Probabilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-1mzF9UA3QP",
        "outputId": "176f85a8-dad3-46d1-cdd0-0a00706f54b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "log_p_tau_filtered = {k: v for k, v in log_p_tau.items()\n",
        "                        if k[1] == '<s>'}\n",
        "\n",
        "topkeys = sorted(log_p_tau_filtered, \n",
        "                 key=log_p_tau_filtered.get, \n",
        "                 reverse=True)[:5]\n",
        "\n",
        "for key in topkeys:\n",
        "    print(key[0], np.exp(log_p_tau[key]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PRP 0.22450589735415977\n",
            "NNP 0.11596270321963643\n",
            "DT 0.11237647433853988\n",
            "IN 0.07810806503028363\n",
            "RB 0.06567580490914879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7J6PMhEhCkf1",
        "outputId": "bdc13a54-d457-4904-9505-f95cbed8646a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "log_p_eta_filtered = {k: v for k, v in log_p_eta.items()\n",
        "                        if k[1] == 'JJ'}\n",
        "\n",
        "topkeys = sorted(log_p_eta_filtered, \n",
        "                 key=log_p_eta_filtered.get, \n",
        "                 reverse=True)[:10]\n",
        "\n",
        "for key in topkeys:\n",
        "    print(key[0], np.exp(log_p_eta[key]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "other 0.023068970800638806\n",
            "good 0.021605646023451937\n",
            "new 0.016785282051542234\n",
            "many 0.013686476641028844\n",
            "great 0.013686476641028844\n",
            "same 0.011362372583143814\n",
            "sure 0.009554736093677673\n",
            "last 0.009210424381398406\n",
            "few 0.008952190597188959\n",
            "little 0.008435723028770062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYw4LpJXDAoC"
      },
      "source": [
        "## 2. Preliminaries for Inference with HMMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-hg6PgxDFwY"
      },
      "source": [
        "### a) Log-probability calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avKga_pnDQsv"
      },
      "source": [
        "Functions to calculate log-probability and accuracy of tagged sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65zvhF9sDxRj"
      },
      "source": [
        "# takes a list of (word, tag) tuples\n",
        "# should start with <s>\n",
        "def log_sentence_probability(s):\n",
        "    total = log_p_tau[('</s>', s[-1][1])]\n",
        "    for i in range(1, len(s)):\n",
        "        total += log_p_tau[(s[i][1], s[i-1][1])]\n",
        "        total += log_p_eta[(s[i][0], s[i][1])] \n",
        "    return total\n",
        "\n",
        "def calc_accuracy_rate(sentences, gold_standards):\n",
        "    total = 0\n",
        "    total_correct = 0\n",
        "    assert(len(sentences) == len(gold_standards))\n",
        "    for s_num in range(len(sentences)):\n",
        "        for w_num in range(len(sentences[s_num])):\n",
        "            total += 1\n",
        "            if sentences[s_num][w_num][1] == gold_standards[s_num][w_num][1]:\n",
        "                total_correct += 1\n",
        "    return total_correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRK9vettmizq"
      },
      "source": [
        "Reusable function to score prediction functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7IBMIy3mXdS"
      },
      "source": [
        "import time\n",
        "\n",
        "# start_work_func prepares all memoization\n",
        "#     takes no args, returns nothing\n",
        "# pred_func does predictions\n",
        "#     takes a sentence of [(word, gs_tag)]\n",
        "#     returns predictions [(word, pred_tag)]\n",
        "def score_prediction_function(start_work_func, pred_func, do_setup=True):\n",
        "    start_time = time.time()\n",
        "    if do_setup:\n",
        "        print('doing setup, building lookup tables...')\n",
        "        start_work_func()\n",
        "    print('making predictions...')\n",
        "    with open('data/en_ewt.dev', 'r') as trainfile:\n",
        "        current_sent = [start_token]\n",
        "        predictions = []\n",
        "        gold_standards = []\n",
        "        total_log_prob = 0\n",
        "        lines = trainfile.readlines()\n",
        "        for row in lines:\n",
        "            row = row.split()\n",
        "            if len(row) == 0:\n",
        "                gold_standards.append(current_sent)\n",
        "                pred = pred_func(current_sent)\n",
        "                log_prob = log_sentence_probability(pred)\n",
        "                predictions.append(pred)\n",
        "                total_log_prob += log_prob\n",
        "                current_sent = [start_token]\n",
        "            else:\n",
        "                current_sent.append(row)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print()\n",
        "    tagging_accuracy = calc_accuracy_rate(predictions, gold_standards)\n",
        "    print('tagging accuracy on DEV:', tagging_accuracy)\n",
        "    print('time required for predictions (s):', elapsed_time)\n",
        "    print('log-probability of predictions:', total_log_prob)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hppzhHqODFMl",
        "outputId": "5d497c53-bb3b-4872-b3c4-852175f4abdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "start_work_func = lambda: None\n",
        "pred_func = lambda x: x\n",
        "\n",
        "score_prediction_function(start_work_func, pred_func)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doing setup, building lookup tables...\n",
            "making predictions...\n",
            "\n",
            "tagging accuracy on DEV: 1.0\n",
            "time required for predictions (s): 0.06492257118225098\n",
            "log-probability of predictions: -169936.39757772826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLK0mhEVQ8vu"
      },
      "source": [
        "### b) Local predictor baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt23Xe2_SOe9",
        "outputId": "81b94502-e53a-4385-f34e-1a381dee871c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "def local_predictor_setup():\n",
        "    global memoize_best_tags\n",
        "    for ix, (k, v) in enumerate(log_p_eta.items()):\n",
        "        if k[0] not in memoize_best_tags:\n",
        "            memoize_best_tags[k[0]] = k[1]\n",
        "        else:\n",
        "            if v > log_p_eta[(k[0], memoize_best_tags[k[0]])]:\n",
        "                memoize_best_tags[k[0]] = k[1]\n",
        "\n",
        "def local_predictor(s):\n",
        "    prediction = [start_token]\n",
        "    for gold_standard in s[1:]:\n",
        "        word = gold_standard[0]\n",
        "        prediction.append((word, memoize_best_tags[word]))\n",
        "    return prediction\n",
        "\n",
        "memoize_best_tags = {}\n",
        "score_prediction_function(local_predictor_setup, local_predictor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doing setup, building lookup tables...\n",
            "making predictions...\n",
            "\n",
            "tagging accuracy on DEV: 0.8137153800824986\n",
            "time required for predictions (s): 1.1839382648468018\n",
            "log-probability of predictions: -186408.12397588338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeyKY48qgnHi"
      },
      "source": [
        "## 3. Greedy Left-to-Right Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keg92OLtgqjH"
      },
      "source": [
        "### a) Implement Greedy Left-to-Right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP0h2Gpeloei",
        "outputId": "55591b0e-9e4d-43c9-b090-2a5a0d74cf40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "G_t_calc = lambda x_t, y, G_tm1: log_p_eta[(x_t, y)] + log_p_tau[(y, G_tm1)]\n",
        "\n",
        "def greedy_ltr_setup():\n",
        "    global G_t_memo\n",
        "    for x_t in vocab:\n",
        "        for G_tm1 in list(pos_tags) + ['<s>']:\n",
        "            max_so_far = -float('inf')\n",
        "            for y in pos_tags:\n",
        "                prob = G_t_calc(x_t, y, G_tm1)\n",
        "                if prob > max_so_far:\n",
        "                    G_t_memo[(x_t, G_tm1)] = y\n",
        "                    max_so_far = prob\n",
        "\n",
        "def greedy_ltr(sentence):\n",
        "    G_t = [start_token]\n",
        "    for word in sentence[1:-1]:\n",
        "        G_t.append((word[0], G_t_memo[(word[0], G_t[-1][1])]))\n",
        "    max_so_far = -float('inf')\n",
        "    x_T = sentence[-1][0]\n",
        "    for y in pos_tags:\n",
        "        prob = G_t_calc(x_T, y, G_t[-1][1]) + log_p_tau[('</s>', y)]\n",
        "        if prob > max_so_far:\n",
        "            final_g = y\n",
        "            max_so_far = prob\n",
        "    G_t.append((x_T, final_g))\n",
        "    return G_t\n",
        "\n",
        "G_t_memo = {}\n",
        "score_prediction_function(greedy_ltr_setup, greedy_ltr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doing setup, building lookup tables...\n",
            "making predictions...\n",
            "\n",
            "tagging accuracy on DEV: 0.8833235120801414\n",
            "time required for predictions (s): 30.3159396648407\n",
            "log-probability of predictions: -166188.89186830737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caCVAmwQsgEQ"
      },
      "source": [
        "## 3. Greedy Right-to-Left Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whtsOnVhskaG"
      },
      "source": [
        "### a) Implement Greedy Right-to-Left"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OWWwgkqsoCi",
        "outputId": "b6ffafc9-7e66-4b03-99ae-7421847a3041",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "R_t_calc = lambda x_t, y, R_tp1: log_p_eta[(x_t, y)] + log_p_tau[(R_tp1, y)]\n",
        "\n",
        "def greedy_rtl_setup():\n",
        "    global R_t_memo\n",
        "    for x_t in vocab:\n",
        "        for R_tp1 in list(pos_tags) + ['</s>']:\n",
        "            max_so_far = -float('inf')\n",
        "            for y in pos_tags:\n",
        "                prob = R_t_calc(x_t, y, R_tp1)\n",
        "                if prob > max_so_far:\n",
        "                    R_t_memo[(x_t, R_tp1)] = y\n",
        "                    max_so_far = prob\n",
        "\n",
        "def greedy_rtl(sentence):\n",
        "    R_t = [end_token]\n",
        "    for i in reversed(sentence[2:]):\n",
        "        R_t.append((i[0], R_t_memo[(i[0], R_t[-1][1])]))\n",
        "    first_y = None\n",
        "    max_val = -float('inf')\n",
        "    for y in pos_tags:\n",
        "        cur_val = log_p_eta[(sentence[1][0], y)] \\\n",
        "                + log_p_tau[(R_t[-1][1], y)]\\\n",
        "                + log_p_tau[(y, '<s>')]\n",
        "        if cur_val > max_val:\n",
        "            max_val = cur_val\n",
        "            first_y = y\n",
        "    R_t.append((sentence[1][0], first_y))\n",
        "    R_t.append(start_token)\n",
        "    return list(reversed(R_t))[:-1]\n",
        "\n",
        "R_t_memo = {}\n",
        "score_prediction_function(greedy_rtl_setup, greedy_rtl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doing setup, building lookup tables...\n",
            "making predictions...\n",
            "\n",
            "tagging accuracy on DEV: 0.8148939304655274\n",
            "time required for predictions (s): 30.373260021209717\n",
            "log-probability of predictions: -176754.90096961826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7md1L9pMwwG4"
      },
      "source": [
        "### b) Results Discussion\n",
        "\n",
        "| Algorithm | Runtime (s) | Tag Accuracy | Log-Prob of Preds |\n",
        "|-----------|-------------|--------------|-------------------|\n",
        "| Local     | 1.23        | 81.37%       | -186408           |\n",
        "| GLTR      | 30.11       | 88.33%       | -166188           |\n",
        "| GRTL      | 30.62       | 81.49%       | -176754           |\n",
        "| Viterbi      | 77.72       | 90.19%       | -163206           |\n",
        "\n",
        " \n",
        "\n",
        "The runtime of GLTR and GRTL were pretty much the same, because they did the exact same amount of work, which was over 50x the work that the local implementation had to do, but for some reason took less than 30x the runtime... Probably has something to do with caching.\n",
        "\n",
        "GRTL did better than local in both log-prob and accuract, because it incorporated more contextual information into its predictions. This is unsurprising.\n",
        "\n",
        "GRTL had noticably lower tag accuracy and log-prob than GLTR. I think this is possibly because the first word in a sentence is pretty likely to be a noun, so the first prediction in GLTR is likely to be correct and will start the algorithm off right. The last thing in a sentence is much more variable, so it's harder to predict, so the odds that the GRTL algorithm will go off the rails right at the start are higher. \n",
        "\n",
        "Also, I think it's possible that because humans understand sentences as they hear them instead of waiting for the sentence to end and then thinking about it, there is probably more \"predictive power\" coming from the previous words in a sentence than the ones after. It makes sense that language would evolve in that way, because it would make it easier to understand sentences as they are said."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRpCAPZt0oCG"
      },
      "source": [
        "## 5. Exact Inference with Viterbi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEnhuOju0r9s",
        "outputId": "561667c7-7936-4264-acda-d8c60e1e2bfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "viterbi_setup = lambda: None\n",
        "\n",
        "def VL(s, t, y):\n",
        "    global V, L\n",
        "    if t == 1:\n",
        "        V[(t, y)] = log_p_eta[(s[1][0], y)] + log_p_tau[(y, '<s>')]\n",
        "        L[(t, y)] = None\n",
        "    elif (t, y) not in V:     \n",
        "        max_Vty = -float('inf')\n",
        "        max_Lty = None\n",
        "        for y_p in pos_tags:\n",
        "            V_tm1_yp, _ = VL(s, t-1, y_p)\n",
        "            val = log_p_eta[(s[t][0], y)] + log_p_tau[(y, y_p)] + V_tm1_yp\n",
        "            if val > max_Vty:\n",
        "                max_Vty = val\n",
        "                max_Lty = y_p\n",
        "        V[(t, y)] = max_Vty\n",
        "        L[(t, y)] = max_Lty\n",
        "    return V[(t, y)], L[(t, y)]\n",
        "    \n",
        "def goal(s):\n",
        "    max_goal = -float('inf')\n",
        "    max_y_hat_t = None\n",
        "    for y_p in pos_tags:\n",
        "        last_v, _ = VL(s, len(s) - 1, y_p)\n",
        "        cur_goal = log_p_tau[('</s>', y_p)] + last_v\n",
        "        if cur_goal > max_goal:\n",
        "            max_goal = cur_goal\n",
        "            max_y_hat_t = y_p\n",
        "    return max_goal, max_y_hat_t\n",
        "    \n",
        "def viterbi_predict(sent):\n",
        "    global V, L\n",
        "    V = {}\n",
        "    L = {}\n",
        "    rev_ys = [(sent[-1][0], goal(sent)[1])]\n",
        "    for ix in reversed(range(1, len(sent) - 1)):\n",
        "        rev_ys.append((sent[ix][0], VL(sent, ix + 1, rev_ys[-1][1])[1]))\n",
        "    return [start_token] + list(reversed(rev_ys))\n",
        "    \n",
        "    \n",
        "score_prediction_function(viterbi_setup, viterbi_predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "doing setup, building lookup tables...\n",
            "making predictions...\n",
            "\n",
            "tagging accuracy on DEV: 0.9019225103123159\n",
            "time required for predictions (s): 77.71943783760071\n",
            "log-probability of predictions: -163206.89965932636\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}